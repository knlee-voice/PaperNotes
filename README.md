# Research Paper Notes 
## PAPER thread - (L)LM , GPT , NLP

* RewardBench: Evaluating Reward Models for Language Modeling - [arXiv/2403.13787](https://arxiv.org/pdf/2403.13787v1.pdf), [Leaderboard](https://huggingface.co/spaces/allenai/reward-bench), [Dataset](https://huggingface.co/datasets/allenai/reward-bench)
* Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge - [arXiv/2403.01432](https://arxiv.org/abs/2403.01432)
* LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models - [arXiv/2403.13372](https://arxiv.org/abs/2403.13372), [Demo Video](https://youtu.be/W29FgeZEpus?si=vhYd_dcGMt5lLTEo), [Code](https://github.com/hiyouga/LLaMA-Factory)
* Training language models to follow instructions with human feedback - [openAI blog](https://openai.com/blog/instruction-following/), [arXiv/2203.02155](https://arxiv.org/abs/2203.02155) 
  - InstructGPT: minimize performance regressions on public NLP datasets by modifying our RLHF fine-tuning procedure.
  - Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.
  - InstructGPT models show improvements in truthfulness over GPT-3 / 3 model sizes (1.3B, 6B, and 175B parameters)
------
* Finetuned Language Models Are Zero-Shot Learners, Google Research - [arXiv/2109.01652](https://arxiv.org/pdf/2109.01652.pdf) 
  - Finetune on many tasks (“instruction-tuning”)
* Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing - [arxiv/2107.13586](https://arxiv.org/pdf/2107.13586v1.pdf)(p.46)
  - #NLP #PLM / Fig: Typology of prompting methods
* Finetuned Language Models Are Zero-Shot Learners - [arxiv/2109.01652](https://arxiv.org/abs/2109.01652)
  - 175B GPT-3 zero-shot/few-shot 비교
* Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems - [arxiv/2108.12589](https://arxiv.org/abs/2108.12589)
* AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing - [arxiv/2108.05542](https://arxiv.org/abs/2108.05542), #ASR #Benchmark
* A Survey on Automated Fact-Checking - [arxiv/2108.11896](https://arxiv.org/abs/2108.11896)
* LoRA: Low-Rank Adaptation of Large Language Models - [arXiv/2106.09685](https://arxiv.org/abs/2106.09685v2). [Code](https://github.com/microsoft/LoRA)
------
* Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks -  [arxiv/2005.11401](https://arxiv.org/abs/2005.11401)
  
## PAPER thread - Foundation Models, Dataset 
* On the opportunity and risks of foundation models - [arxiv/2108.07258](https://arxiv.org/abs/2108.07258)(p.160)
* Datasets: A Community Library for Natural Language Processing - [arxiv/2109.02846](https://arxiv.org/pdf/2109.02846.pdf)
 
## PAPER thread - ASR
* ASR-GLUE: A New Multi-task Benchmark for ASR-Robust Natural Language Understanding - [arxiv/2108.13048](https://arxiv.org/abs/2108.13048), #Benchmark
* SpecAugment: A New Data Augmentation Method for Automatic Speech Recognition [[arXiv](https://arxiv.org/abs/1904.08779)], [[google AI blog](https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html)] 
* Advances in All-Neural Speech Recognition [[arXiv/1609.05935](https://arxiv.org/abs/1609.05935)], [[note](https://github.com/knlee-voice/PaperNotes/blob/master/notes/aXv1609.05935.md)]
* Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling, 
* Personal Speech Recognition on Mobile Devices [[arXiv/1603.03185](https://arxiv.org/abs/1603.03185)], [[note](notes/aXv1603.03185.md)]
* Textless NLP: Generating expressive speech from raw audio - [facebook AI](https://ai.facebook.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio)

---
# Links 
https://paperswithcode.com/
