# Research Paper Notes 
## PAPER threads - (L)LM , GPT , NLP
* RewardBench: Evaluating Reward Models for Language Modeling - [arXiv/2403.13787](https://arxiv.org/pdf/2403.13787v1.pdf), [Leaderboard](https://huggingface.co/spaces/allenai/reward-bench), [Dataset](https://huggingface.co/datasets/allenai/reward-bench)
* Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge - [arXiv/2403.01432](https://arxiv.org/abs/2403.01432)
* LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models - [arXiv/2403.13372](https://arxiv.org/abs/2403.13372), [Demo Video](https://youtu.be/W29FgeZEpus?si=vhYd_dcGMt5lLTEo), [Code](https://github.com/hiyouga/LLaMA-Factory)
* Training language models to follow instructions with human feedback - [arXiv/2203.02155](https://arxiv.org/abs/2203.02155), [openAI blog](https://openai.com/blog/instruction-following/)
  - InstructGPT: minimize performance regressions on public NLP datasets by modifying our RLHF fine-tuning procedure.
  - Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.
  - InstructGPT models show improvements in truthfulness over GPT-3 / 3 model sizes (1.3B, 6B, and 175B parameters)
------
* Finetuned Language Models Are Zero-Shot Learners, Google Research - [arXiv/2109.01652](https://arxiv.org/pdf/2109.01652.pdf) 
  - Finetune on many tasks (“instruction-tuning”)
* Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing - [arXiv/2107.13586](https://arxiv.org/pdf/2107.13586v1.pdf)(p.46)
  - #NLP #PLM / Fig: Typology of prompting methods
* Finetuned Language Models Are Zero-Shot Learners - [arXiv/2109.01652](https://arxiv.org/abs/2109.01652)
  - 175B GPT-3 zero-shot/few-shot 비교
* Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems - [arXiv/2108.12589](https://arxiv.org/abs/2108.12589)
* AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing - [arXiv/2108.05542](https://arxiv.org/abs/2108.05542), #ASR #Benchmark
* A Survey on Automated Fact-Checking - [arXiv/2108.11896](https://arxiv.org/abs/2108.11896)
* LoRA: Low-Rank Adaptation of Large Language Models - [arXiv/2106.09685](https://arxiv.org/abs/2106.09685v2), [Code](https://github.com/microsoft/LoRA)
------
* Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks -  [arXiv/2005.11401](https://arxiv.org/abs/2005.11401)
* Longformer: The Long-Document Transformer - [arXiv/2004.05150](https://arxiv.org/abs/2004.05150)

## PAPER threads - Agent 
* AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System - [arXiv/2402.15538](https://arxiv.org/abs/2402.15538), [Fig.Illustration](https://d3i71xaburhd42.cloudfront.net/fc1dec23e44b7316cae4cda93ab0fdd1c56b2f28/9-Figure2-1.png)
  
## PAPER threads - Foundation Models, Dataset 
* On the opportunity and risks of foundation models - [arXiv/2108.07258](https://arxiv.org/abs/2108.07258)(p.160)
* Datasets: A Community Library for Natural Language Processing - [arXiv/2109.02846](https://arxiv.org/pdf/2109.02846.pdf)
 
## PAPER threads - ASR, TTS, #Voice
* OpenVoice: Versatile Instant Voice Cloning - [arXiv/2312.01479](https://arxiv.org/abs/2312.01479), https://research.myshell.ai/open-voice
  - (Accurate Tone Color Cloning/Flexible Voice Style Control/Zero-shot Cross-lingual Voice Cloning)
* VALL-E X: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling - [arXiv/2303.03926](https://arxiv.org/abs/2303.03926)
  - pre-train(60K hours) / [VALL-E_논문리뷰](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/vall-e/)
* ASR-GLUE: A New Multi-task Benchmark for ASR-Robust Natural Language Understanding - [arXiv/2108.13048](https://arxiv.org/abs/2108.13048), #Benchmark
* SpecAugment: A New Data Augmentation Method for Automatic Speech Recognition [[arXiv](https://arxiv.org/abs/1904.08779)], [[google AI blog](https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html)] 
* Advances in All-Neural Speech Recognition [[arXiv/1609.05935](https://arxiv.org/abs/1609.05935)], [[note](https://github.com/knlee-voice/PaperNotes/blob/master/notes/aXv1609.05935.md)]
* Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling, 
* Personal Speech Recognition on Mobile Devices [[arXiv/1603.03185](https://arxiv.org/abs/1603.03185)], [[note](notes/aXv1603.03185.md)]
* Textless NLP: Generating expressive speech from raw audio - [facebook AI](https://ai.facebook.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio)

---
# Links 
https://paperswithcode.com/
